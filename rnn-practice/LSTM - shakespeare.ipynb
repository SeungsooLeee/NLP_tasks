{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM을 통해 셰익스피어 작품을 학습한 후 문장의 다음 단어를 예측 (\n",
    "machine learning cookbook \n",
    "#### import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import requests\n",
    "import numpy as np\n",
    "import collections\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN변수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "min_word_freq = 5         #빈도가 낮은 단어 제외\n",
    "rnn_size = 128            #rnn 모델 크기, 임베딩 크기와 동일\n",
    "epochs = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.001     #학습률\n",
    "training_seq_len = 50     # 고려 대상인 단어 묶음 길이 \n",
    "embedding_size = rnn_size\n",
    "save_every = 500          #모델 확인 및 저장 주기 \n",
    "eval_every = 50           # 테스트 문장 평가 주기\n",
    "prime_texts = ['thou art more' ,'to be or not to', 'wherefore art thou']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Shakespeare Data\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'temp_shakespeare'\n",
    "data_file = 'shakespeare.txt'\n",
    "model_path = 'shakespeare_model'\n",
    "full_model_dir = os.path.join(data_dir, model_path)\n",
    "\n",
    "punctuation = string.punctuation\n",
    "punctuation = ''.join([x for x in punctuation if x not in ['-', \"'\"]])\n",
    "\n",
    "if not os.path.exists(full_model_dir):\n",
    "    os.makedirs(full_model_dir)\n",
    "    \n",
    "#데이터 디렉토리 생성\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "print ('Loading Shakespeare Data')\n",
    "\n",
    "#파일 다운로드 여부 확인\n",
    "if not os.path.isfile(os.path.join(data_dir, data_file)):\n",
    "    print ('Not found, downloading Shakespeare texts from www..gutenberg.org')\n",
    "    \n",
    "    shakespeare_url = \"http://www.gutenberg.org/chche/epub/100/pg100.txt\"\n",
    "    \n",
    "    #셰익스피어 문서 받기\n",
    "    response = requests.get(shakespeare_url)\n",
    "    shakespeare_file = response.content\n",
    "    \n",
    "    #이진 문자열 변환\n",
    "    s_text = shakespeare_file.decode('utf-8')\n",
    "    #초반 설명 단락 제거\n",
    "    s_text =s_text[7675:]\n",
    "    #개행 문자 제거 \n",
    "    s_text = s_text.replace('\\r\\n',\"\")\n",
    "    s_text = s_text.replace('\\n',\"\")\n",
    "\n",
    "    #파일 저장\n",
    "    with open(os.path.join(data_dir, data_file), 'w') as out_conn:\n",
    "        out_conn.write(s_text)\n",
    "else:\n",
    "    #저장된 파일이 있으면 해당 파일 로드\n",
    "    with open(os.path.join(data_dir, data_file), 'r') as file_conn:\n",
    "        s_text = file_conn.read().replace('\\n','')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 기호 및 공백 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s_text = re.sub(r'[{}]'.format(punctuation), ' ', s_text)\n",
    "s_text = re.sub('\\s+', ' ', s_text).strip().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 어휘사전 생성 \n",
    "\n",
    "특정 빈도 이상 출현하는 단어들에 대해 두종류의 사전 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(text,min_word_freq):\n",
    "    word_count = collections.Counter(text.split(' '))\n",
    "    \n",
    "    #특정 값보다 빈도가 높은 경우 빈도 값의 상한을 설정\n",
    "    word_count = {key:val for key, val in word_count.items() if val > min_word_freq}\n",
    "    \n",
    "    #단어 -> 색인값대응\n",
    "    words = word_count.keys()\n",
    "    vocab_to_ix_dict = {key:(ix+1) for ix, key in enumerate(words)} # 1부터 시작\n",
    "    \n",
    "    #알 수 없음에 해당하는 색인 값 0 추가\n",
    "    vocab_to_ix_dict['unknown']=0\n",
    "    \n",
    "    #색인 값 -> 단어 대응\n",
    "    ix_to_vocab_dict = {val:key for key, val in vocab_to_ix_dict.items()}\n",
    "    \n",
    "    return (ix_to_vocab_dict, vocab_to_ix_dict)\n",
    "\n",
    "ix2vocab, vocab2ix = build_vocab(s_text, min_word_freq)\n",
    "vocab_size = len(ix2vocab) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문서를 색인 값 배열로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s_text_words = s_text.split(' ')\n",
    "s_text_ix =[]\n",
    "for ix, x in enumerate(s_text_words):\n",
    "    try:\n",
    "        s_text_ix.append(vocab2ix[x])\n",
    "    except:\n",
    "        s_text_ix.append(0)\n",
    "s_text_ix = np.array(s_text_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM RNN Model\n",
    "class LSTM_Model():\n",
    "    def __init__(self, embedding_size, rnn_size, batch_size, learning_rate,\n",
    "                 training_seq_len, vocab_size, infer_sample=False):\n",
    "        self.embedding_size = embedding_size\n",
    "        self.rnn_size = rnn_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.infer_sample = infer_sample\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        if infer_sample:\n",
    "            self.batch_size = 1\n",
    "            self.training_seq_len = 1\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "            self.training_seq_len = training_seq_len\n",
    "        \n",
    "        self.lstm_cell = tf.contrib.rnn.BasicLSTMCell(self.rnn_size)\n",
    "        self.initial_state = self.lstm_cell.zero_state(self.batch_size, tf.float32)\n",
    "        \n",
    "        self.x_data = tf.placeholder(tf.int32, [self.batch_size, self.training_seq_len])\n",
    "        self.y_output = tf.placeholder(tf.int32, [self.batch_size, self.training_seq_len])\n",
    "        \n",
    "        with tf.variable_scope('lstm_vars'):\n",
    "            # Softmax Output Weights\n",
    "            W = tf.get_variable('W', [self.rnn_size, self.vocab_size], tf.float32, tf.random_normal_initializer())\n",
    "            b = tf.get_variable('b', [self.vocab_size], tf.float32, tf.constant_initializer(0.0))\n",
    "        \n",
    "            # Define Embedding\n",
    "            embedding_mat = tf.get_variable('embedding_mat', [self.vocab_size, self.embedding_size],\n",
    "                                            tf.float32, tf.random_normal_initializer())\n",
    "                                            \n",
    "            embedding_output = tf.nn.embedding_lookup(embedding_mat, self.x_data)\n",
    "            rnn_inputs = tf.split(axis=1, num_or_size_splits=self.training_seq_len, value=embedding_output)\n",
    "            rnn_inputs_trimmed = [tf.squeeze(x, [1]) for x in rnn_inputs]\n",
    "        \n",
    "        # If we are inferring (generating text), we add a 'loop' function\n",
    "        # Define how to get the i+1 th input from the i th output\n",
    "        def inferred_loop(prev, count):\n",
    "            # Apply hidden layer\n",
    "            prev_transformed = tf.matmul(prev, W) + b\n",
    "            # Get the index of the output (also don't run the gradient)\n",
    "            prev_symbol = tf.stop_gradient(tf.argmax(prev_transformed, 1))\n",
    "            # Get embedded vector\n",
    "            output = tf.nn.embedding_lookup(embedding_mat, prev_symbol)\n",
    "            return(output)\n",
    "        \n",
    "        decoder = tf.contrib.legacy_seq2seq.rnn_decoder\n",
    "        outputs, last_state = decoder(rnn_inputs_trimmed,\n",
    "                                      self.initial_state,\n",
    "                                      self.lstm_cell,\n",
    "                                      loop_function=inferred_loop if infer_sample else None)\n",
    "        # Non inferred outputs\n",
    "        output = tf.reshape(tf.concat(axis=1, values=outputs), [-1, self.rnn_size])\n",
    "        # Logits and output\n",
    "        self.logit_output = tf.matmul(output, W) + b\n",
    "        self.model_output = tf.nn.softmax(self.logit_output)\n",
    "        \n",
    "        loss_fun = tf.contrib.legacy_seq2seq.sequence_loss_by_example\n",
    "        loss = loss_fun([self.logit_output],[tf.reshape(self.y_output, [-1])],\n",
    "                [tf.ones([self.batch_size * self.training_seq_len])],\n",
    "                self.vocab_size)\n",
    "        self.cost = tf.reduce_sum(loss) / (self.batch_size * self.training_seq_len)\n",
    "        self.final_state = last_state\n",
    "        gradients, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tf.trainable_variables()), 4.5)\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        self.train_op = optimizer.apply_gradients(zip(gradients, tf.trainable_variables()))\n",
    "        \n",
    "    def sample(self, sess, words=ix2vocab, vocab=vocab2ix, num=10, prime_text='thou art'):\n",
    "        state = sess.run(self.lstm_cell.zero_state(1, tf.float32))\n",
    "        word_list = prime_text.split()\n",
    "        for word in word_list[:-1]:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab[word]\n",
    "            feed_dict = {self.x_data: x, self.initial_state:state}\n",
    "            [state] = sess.run([self.final_state], feed_dict=feed_dict)\n",
    "\n",
    "        out_sentence = prime_text\n",
    "        word = word_list[-1]\n",
    "        for n in range(num):\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[word]\n",
    "            feed_dict = {self.x_data: x, self.initial_state:state}\n",
    "            [model_output, state] = sess.run([self.model_output, self.final_state], feed_dict=feed_dict)\n",
    "            sample = np.argmax(model_output[0])\n",
    "            if sample == 0:\n",
    "                break\n",
    "            word = words[sample]\n",
    "            out_sentence = out_sentence + ' ' + word\n",
    "        return(out_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 테스트모델, LSTM모델 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM Model\n",
    "lstm_model = LSTM_Model(embedding_size, rnn_size, batch_size, learning_rate,\n",
    "                        training_seq_len, vocab_size)\n",
    "\n",
    "# Tell TensorFlow we are reusing the scope for the testing\n",
    "with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n",
    "    test_lstm_model = LSTM_Model(embedding_size, rnn_size, batch_size, learning_rate,\n",
    "                                 training_seq_len, vocab_size, infer_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "# Create batches for each epoch\n",
    "num_batches = int(len(s_text_ix)/(batch_size * training_seq_len)) + 1\n",
    "# Split up text indices into subarrays, of equal size\n",
    "batches = np.array_split(s_text_ix, num_batches)\n",
    "# Reshape each split into [batch_size, training_seq_len]\n",
    "batches = [np.resize(x, [batch_size, training_seq_len]) for x in batches]\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch #1 of 10.\n",
      "Iteration: 10, Epoch: 1, Batch: 10 out of 182, Loss: 7.51\n",
      "Iteration: 20, Epoch: 1, Batch: 20 out of 182, Loss: 7.29\n",
      "Iteration: 30, Epoch: 1, Batch: 30 out of 182, Loss: 7.26\n",
      "Iteration: 40, Epoch: 1, Batch: 40 out of 182, Loss: 7.07\n",
      "Iteration: 50, Epoch: 1, Batch: 50 out of 182, Loss: 6.97\n",
      "thou art more than of the\n",
      "to be or not to be\n",
      "wherefore art thou\n",
      "Iteration: 60, Epoch: 1, Batch: 60 out of 182, Loss: 6.80\n",
      "Iteration: 70, Epoch: 1, Batch: 70 out of 182, Loss: 6.86\n",
      "Iteration: 80, Epoch: 1, Batch: 80 out of 182, Loss: 6.85\n",
      "Iteration: 90, Epoch: 1, Batch: 90 out of 182, Loss: 6.62\n",
      "Iteration: 100, Epoch: 1, Batch: 100 out of 182, Loss: 6.76\n",
      "thou art more than\n",
      "to be or not to be\n",
      "wherefore art thou art thou\n",
      "Iteration: 110, Epoch: 1, Batch: 110 out of 182, Loss: 6.47\n",
      "Iteration: 120, Epoch: 1, Batch: 120 out of 182, Loss: 6.34\n",
      "Iteration: 130, Epoch: 1, Batch: 130 out of 182, Loss: 6.52\n",
      "Iteration: 140, Epoch: 1, Batch: 140 out of 182, Loss: 6.42\n",
      "Iteration: 150, Epoch: 1, Batch: 150 out of 182, Loss: 6.49\n",
      "thou art more than of the\n",
      "to be or not to the\n",
      "wherefore art thou art thou art thou art thou art thou art thou\n",
      "Iteration: 160, Epoch: 1, Batch: 160 out of 182, Loss: 6.32\n",
      "Iteration: 170, Epoch: 1, Batch: 170 out of 182, Loss: 6.35\n",
      "Iteration: 180, Epoch: 1, Batch: 180 out of 182, Loss: 6.31\n",
      "Starting Epoch #2 of 10.\n",
      "Iteration: 190, Epoch: 2, Batch: 9 out of 182, Loss: 6.78\n",
      "Iteration: 200, Epoch: 2, Batch: 19 out of 182, Loss: 6.96\n",
      "thou art more than of the\n",
      "to be or not to the\n",
      "wherefore art thou art thou art thou art thou art thou art thou\n",
      "Iteration: 210, Epoch: 2, Batch: 29 out of 182, Loss: 6.58\n",
      "Iteration: 220, Epoch: 2, Batch: 39 out of 182, Loss: 6.38\n",
      "Iteration: 230, Epoch: 2, Batch: 49 out of 182, Loss: 6.50\n",
      "Iteration: 240, Epoch: 2, Batch: 59 out of 182, Loss: 6.19\n",
      "Iteration: 250, Epoch: 2, Batch: 69 out of 182, Loss: 6.49\n",
      "thou art more than of the\n",
      "to be or not to be\n",
      "wherefore art thou art thou art thou art thou art thou art thou\n",
      "Iteration: 260, Epoch: 2, Batch: 79 out of 182, Loss: 6.45\n",
      "Iteration: 270, Epoch: 2, Batch: 89 out of 182, Loss: 6.36\n",
      "Iteration: 280, Epoch: 2, Batch: 99 out of 182, Loss: 6.43\n",
      "Iteration: 290, Epoch: 2, Batch: 109 out of 182, Loss: 6.16\n",
      "Iteration: 300, Epoch: 2, Batch: 119 out of 182, Loss: 6.28\n",
      "thou art more than of his\n",
      "to be or not to be\n",
      "wherefore art thou art thou art thou art thou art thou art thou\n",
      "Iteration: 310, Epoch: 2, Batch: 129 out of 182, Loss: 6.11\n",
      "Iteration: 320, Epoch: 2, Batch: 139 out of 182, Loss: 6.28\n",
      "Iteration: 330, Epoch: 2, Batch: 149 out of 182, Loss: 6.10\n",
      "Iteration: 340, Epoch: 2, Batch: 159 out of 182, Loss: 6.28\n",
      "Iteration: 350, Epoch: 2, Batch: 169 out of 182, Loss: 6.38\n",
      "thou art more than than the\n",
      "to be or not to the\n",
      "wherefore art thou art thou art thou art thou art thou art thou\n",
      "Iteration: 360, Epoch: 2, Batch: 179 out of 182, Loss: 6.10\n",
      "Starting Epoch #3 of 10.\n",
      "Iteration: 370, Epoch: 3, Batch: 8 out of 182, Loss: 6.38\n",
      "Iteration: 380, Epoch: 3, Batch: 18 out of 182, Loss: 6.14\n",
      "Iteration: 390, Epoch: 3, Batch: 28 out of 182, Loss: 6.25\n",
      "Iteration: 400, Epoch: 3, Batch: 38 out of 182, Loss: 6.13\n",
      "thou art more than of the\n",
      "to be or not to the\n",
      "wherefore art thou art thou art thou art thou art thou\n",
      "Iteration: 410, Epoch: 3, Batch: 48 out of 182, Loss: 6.20\n",
      "Iteration: 420, Epoch: 3, Batch: 58 out of 182, Loss: 6.29\n",
      "Iteration: 430, Epoch: 3, Batch: 68 out of 182, Loss: 6.10\n",
      "Iteration: 440, Epoch: 3, Batch: 78 out of 182, Loss: 5.94\n",
      "Iteration: 450, Epoch: 3, Batch: 88 out of 182, Loss: 6.25\n",
      "thou art more than i am\n",
      "to be or not to the\n",
      "wherefore art thou art thou art thou\n",
      "Iteration: 460, Epoch: 3, Batch: 98 out of 182, Loss: 6.08\n",
      "Iteration: 470, Epoch: 3, Batch: 108 out of 182, Loss: 6.15\n",
      "Iteration: 480, Epoch: 3, Batch: 118 out of 182, Loss: 6.20\n",
      "Iteration: 490, Epoch: 3, Batch: 128 out of 182, Loss: 6.17\n",
      "Iteration: 500, Epoch: 3, Batch: 138 out of 182, Loss: 6.10\n",
      "Model Saved To: temp/shakespeare_model/model\n",
      "thou art more than a\n",
      "to be or not to be\n",
      "wherefore art thou art thou art thou\n",
      "Iteration: 510, Epoch: 3, Batch: 148 out of 182, Loss: 6.04\n",
      "Iteration: 520, Epoch: 3, Batch: 158 out of 182, Loss: 6.12\n",
      "Iteration: 530, Epoch: 3, Batch: 168 out of 182, Loss: 6.20\n",
      "Iteration: 540, Epoch: 3, Batch: 178 out of 182, Loss: 6.30\n",
      "Starting Epoch #4 of 10.\n",
      "Iteration: 550, Epoch: 4, Batch: 7 out of 182, Loss: 6.27\n",
      "thou art more than of the\n",
      "to be or not to the\n",
      "wherefore art thou art thou art thou\n",
      "Iteration: 560, Epoch: 4, Batch: 17 out of 182, Loss: 6.12\n",
      "Iteration: 570, Epoch: 4, Batch: 27 out of 182, Loss: 5.90\n",
      "Iteration: 580, Epoch: 4, Batch: 37 out of 182, Loss: 6.00\n",
      "Iteration: 590, Epoch: 4, Batch: 47 out of 182, Loss: 6.02\n",
      "Iteration: 600, Epoch: 4, Batch: 57 out of 182, Loss: 6.18\n",
      "thou art more than a\n",
      "to be or not to be\n",
      "wherefore art thou art thou art thou art thou\n",
      "Iteration: 610, Epoch: 4, Batch: 67 out of 182, Loss: 6.02\n",
      "Iteration: 620, Epoch: 4, Batch: 77 out of 182, Loss: 6.06\n",
      "Iteration: 630, Epoch: 4, Batch: 87 out of 182, Loss: 6.07\n",
      "Iteration: 640, Epoch: 4, Batch: 97 out of 182, Loss: 6.02\n",
      "Iteration: 650, Epoch: 4, Batch: 107 out of 182, Loss: 6.01\n",
      "thou art more than a\n",
      "to be or not to be\n",
      "wherefore art thou art thou art thou art thou art thou\n",
      "Iteration: 660, Epoch: 4, Batch: 117 out of 182, Loss: 5.99\n",
      "Iteration: 670, Epoch: 4, Batch: 127 out of 182, Loss: 6.22\n",
      "Iteration: 680, Epoch: 4, Batch: 137 out of 182, Loss: 5.78\n",
      "Iteration: 690, Epoch: 4, Batch: 147 out of 182, Loss: 6.29\n",
      "Iteration: 700, Epoch: 4, Batch: 157 out of 182, Loss: 5.92\n",
      "thou art more than a\n",
      "to be or not to be\n",
      "wherefore art thou art thou art thou art thou art thou art thou\n",
      "Iteration: 710, Epoch: 4, Batch: 167 out of 182, Loss: 6.09\n",
      "Iteration: 720, Epoch: 4, Batch: 177 out of 182, Loss: 6.15\n",
      "Starting Epoch #5 of 10.\n",
      "Iteration: 730, Epoch: 5, Batch: 6 out of 182, Loss: 6.03\n",
      "Iteration: 740, Epoch: 5, Batch: 16 out of 182, Loss: 6.13\n",
      "Iteration: 750, Epoch: 5, Batch: 26 out of 182, Loss: 6.03\n",
      "thou art more than a\n",
      "to be or not to be\n",
      "wherefore art thou art a\n",
      "Iteration: 760, Epoch: 5, Batch: 36 out of 182, Loss: 6.20\n",
      "Iteration: 770, Epoch: 5, Batch: 46 out of 182, Loss: 6.03\n",
      "Iteration: 780, Epoch: 5, Batch: 56 out of 182, Loss: 5.99\n",
      "Iteration: 790, Epoch: 5, Batch: 66 out of 182, Loss: 5.93\n",
      "Iteration: 800, Epoch: 5, Batch: 76 out of 182, Loss: 5.90\n",
      "thou art more than a\n",
      "to be or not to be\n",
      "wherefore art thou art thou art a\n",
      "Iteration: 810, Epoch: 5, Batch: 86 out of 182, Loss: 6.13\n",
      "Iteration: 820, Epoch: 5, Batch: 96 out of 182, Loss: 5.95\n",
      "Iteration: 830, Epoch: 5, Batch: 106 out of 182, Loss: 6.25\n",
      "Iteration: 840, Epoch: 5, Batch: 116 out of 182, Loss: 6.10\n",
      "Iteration: 850, Epoch: 5, Batch: 126 out of 182, Loss: 5.93\n",
      "thou art more than a\n",
      "to be or not to be\n",
      "wherefore art thou art thou art a\n",
      "Iteration: 860, Epoch: 5, Batch: 136 out of 182, Loss: 5.93\n",
      "Iteration: 870, Epoch: 5, Batch: 146 out of 182, Loss: 6.21\n",
      "Iteration: 880, Epoch: 5, Batch: 156 out of 182, Loss: 6.12\n",
      "Iteration: 890, Epoch: 5, Batch: 166 out of 182, Loss: 6.02\n",
      "Iteration: 900, Epoch: 5, Batch: 176 out of 182, Loss: 5.95\n",
      "thou art more than a\n",
      "to be or not to be\n",
      "wherefore art thou art not\n",
      "Starting Epoch #6 of 10.\n",
      "Iteration: 910, Epoch: 6, Batch: 5 out of 182, Loss: 5.98\n",
      "Iteration: 920, Epoch: 6, Batch: 15 out of 182, Loss: 5.86\n",
      "Iteration: 930, Epoch: 6, Batch: 25 out of 182, Loss: 5.85\n",
      "Iteration: 940, Epoch: 6, Batch: 35 out of 182, Loss: 5.84\n",
      "Iteration: 950, Epoch: 6, Batch: 45 out of 182, Loss: 5.96\n",
      "thou art more than a\n",
      "to be or not to be\n",
      "wherefore art thou art a\n",
      "Iteration: 960, Epoch: 6, Batch: 55 out of 182, Loss: 5.97\n",
      "Iteration: 970, Epoch: 6, Batch: 65 out of 182, Loss: 5.96\n",
      "Iteration: 980, Epoch: 6, Batch: 75 out of 182, Loss: 5.96\n",
      "Iteration: 990, Epoch: 6, Batch: 85 out of 182, Loss: 6.14\n",
      "Iteration: 1000, Epoch: 6, Batch: 95 out of 182, Loss: 5.71\n",
      "Model Saved To: temp/shakespeare_model/model\n",
      "thou art more than a\n",
      "to be or not to be\n",
      "wherefore art thou art a\n",
      "Iteration: 1010, Epoch: 6, Batch: 105 out of 182, Loss: 5.73\n",
      "Iteration: 1020, Epoch: 6, Batch: 115 out of 182, Loss: 6.04\n",
      "Iteration: 1030, Epoch: 6, Batch: 125 out of 182, Loss: 5.62\n",
      "Iteration: 1040, Epoch: 6, Batch: 135 out of 182, Loss: 5.99\n",
      "Iteration: 1050, Epoch: 6, Batch: 145 out of 182, Loss: 5.77\n",
      "thou art more than a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to be or not to be\n",
      "wherefore art thou art a\n",
      "Iteration: 1060, Epoch: 6, Batch: 155 out of 182, Loss: 6.06\n",
      "Iteration: 1070, Epoch: 6, Batch: 165 out of 182, Loss: 5.75\n",
      "Iteration: 1080, Epoch: 6, Batch: 175 out of 182, Loss: 6.05\n",
      "Starting Epoch #7 of 10.\n",
      "Iteration: 1090, Epoch: 7, Batch: 4 out of 182, Loss: 5.81\n",
      "Iteration: 1100, Epoch: 7, Batch: 14 out of 182, Loss: 5.82\n",
      "thou art more than a\n",
      "to be or not to be\n",
      "wherefore art thou art a\n",
      "Iteration: 1110, Epoch: 7, Batch: 24 out of 182, Loss: 5.89\n",
      "Iteration: 1120, Epoch: 7, Batch: 34 out of 182, Loss: 5.77\n",
      "Iteration: 1130, Epoch: 7, Batch: 44 out of 182, Loss: 5.98\n",
      "Iteration: 1140, Epoch: 7, Batch: 54 out of 182, Loss: 5.81\n",
      "Iteration: 1150, Epoch: 7, Batch: 64 out of 182, Loss: 5.97\n",
      "thou art more than a\n",
      "to be or not to be\n",
      "wherefore art thou art a\n",
      "Iteration: 1160, Epoch: 7, Batch: 74 out of 182, Loss: 6.02\n",
      "Iteration: 1170, Epoch: 7, Batch: 84 out of 182, Loss: 5.77\n",
      "Iteration: 1180, Epoch: 7, Batch: 94 out of 182, Loss: 5.87\n",
      "Iteration: 1190, Epoch: 7, Batch: 104 out of 182, Loss: 5.78\n",
      "Iteration: 1200, Epoch: 7, Batch: 114 out of 182, Loss: 5.85\n",
      "thou art more than a\n",
      "to be or not to be\n",
      "wherefore art thou art a\n",
      "Iteration: 1210, Epoch: 7, Batch: 124 out of 182, Loss: 6.00\n",
      "Iteration: 1220, Epoch: 7, Batch: 134 out of 182, Loss: 5.60\n",
      "Iteration: 1230, Epoch: 7, Batch: 144 out of 182, Loss: 5.91\n",
      "Iteration: 1240, Epoch: 7, Batch: 154 out of 182, Loss: 5.76\n",
      "Iteration: 1250, Epoch: 7, Batch: 164 out of 182, Loss: 5.84\n",
      "thou art more than a\n",
      "to be or not to be\n",
      "wherefore art thou art a\n",
      "Iteration: 1260, Epoch: 7, Batch: 174 out of 182, Loss: 6.11\n",
      "Starting Epoch #8 of 10.\n",
      "Iteration: 1270, Epoch: 8, Batch: 3 out of 182, Loss: 5.62\n",
      "Iteration: 1280, Epoch: 8, Batch: 13 out of 182, Loss: 5.71\n",
      "Iteration: 1290, Epoch: 8, Batch: 23 out of 182, Loss: 5.74\n",
      "Iteration: 1300, Epoch: 8, Batch: 33 out of 182, Loss: 5.77\n",
      "thou art more than a\n",
      "to be or not to be\n",
      "wherefore art thou art a\n",
      "Iteration: 1310, Epoch: 8, Batch: 43 out of 182, Loss: 5.66\n",
      "Iteration: 1320, Epoch: 8, Batch: 53 out of 182, Loss: 5.90\n",
      "Iteration: 1330, Epoch: 8, Batch: 63 out of 182, Loss: 6.01\n",
      "Iteration: 1340, Epoch: 8, Batch: 73 out of 182, Loss: 5.88\n",
      "Iteration: 1350, Epoch: 8, Batch: 83 out of 182, Loss: 5.79\n",
      "thou art more than a\n",
      "to be or not to be\n",
      "wherefore art thou art a\n",
      "Iteration: 1360, Epoch: 8, Batch: 93 out of 182, Loss: 5.73\n",
      "Iteration: 1370, Epoch: 8, Batch: 103 out of 182, Loss: 5.76\n",
      "Iteration: 1380, Epoch: 8, Batch: 113 out of 182, Loss: 5.73\n",
      "Iteration: 1390, Epoch: 8, Batch: 123 out of 182, Loss: 5.82\n",
      "Iteration: 1400, Epoch: 8, Batch: 133 out of 182, Loss: 5.98\n",
      "thou art more than a\n",
      "to be or not to be\n",
      "wherefore art thou art a\n",
      "Iteration: 1410, Epoch: 8, Batch: 143 out of 182, Loss: 5.81\n",
      "Iteration: 1420, Epoch: 8, Batch: 153 out of 182, Loss: 5.51\n",
      "Iteration: 1430, Epoch: 8, Batch: 163 out of 182, Loss: 5.84\n",
      "Iteration: 1440, Epoch: 8, Batch: 173 out of 182, Loss: 5.62\n",
      "Starting Epoch #9 of 10.\n",
      "Iteration: 1450, Epoch: 9, Batch: 2 out of 182, Loss: 5.55\n",
      "thou art more than a\n",
      "to be or not to be\n",
      "wherefore art thou art a\n",
      "Iteration: 1460, Epoch: 9, Batch: 12 out of 182, Loss: 5.95\n",
      "Iteration: 1470, Epoch: 9, Batch: 22 out of 182, Loss: 5.79\n",
      "Iteration: 1480, Epoch: 9, Batch: 32 out of 182, Loss: 5.64\n",
      "Iteration: 1490, Epoch: 9, Batch: 42 out of 182, Loss: 5.71\n",
      "Iteration: 1500, Epoch: 9, Batch: 52 out of 182, Loss: 5.85\n",
      "Model Saved To: temp/shakespeare_model/model\n",
      "thou art more than a\n",
      "to be or not to be\n",
      "wherefore art thou art a\n",
      "Iteration: 1510, Epoch: 9, Batch: 62 out of 182, Loss: 5.87\n",
      "Iteration: 1520, Epoch: 9, Batch: 72 out of 182, Loss: 5.92\n",
      "Iteration: 1530, Epoch: 9, Batch: 82 out of 182, Loss: 5.81\n",
      "Iteration: 1540, Epoch: 9, Batch: 92 out of 182, Loss: 6.03\n",
      "Iteration: 1550, Epoch: 9, Batch: 102 out of 182, Loss: 5.51\n",
      "thou art more than the\n",
      "to be or not to be\n",
      "wherefore art thou art a\n",
      "Iteration: 1560, Epoch: 9, Batch: 112 out of 182, Loss: 6.00\n",
      "Iteration: 1570, Epoch: 9, Batch: 122 out of 182, Loss: 5.95\n",
      "Iteration: 1580, Epoch: 9, Batch: 132 out of 182, Loss: 5.86\n",
      "Iteration: 1590, Epoch: 9, Batch: 142 out of 182, Loss: 5.62\n",
      "Iteration: 1600, Epoch: 9, Batch: 152 out of 182, Loss: 5.82\n",
      "thou art more than a\n",
      "to be or not to be\n",
      "wherefore art thou art a\n",
      "Iteration: 1610, Epoch: 9, Batch: 162 out of 182, Loss: 5.70\n",
      "Iteration: 1620, Epoch: 9, Batch: 172 out of 182, Loss: 5.73\n",
      "Starting Epoch #10 of 10.\n",
      "Iteration: 1630, Epoch: 10, Batch: 1 out of 182, Loss: 5.92\n",
      "Iteration: 1640, Epoch: 10, Batch: 11 out of 182, Loss: 5.75\n",
      "Iteration: 1650, Epoch: 10, Batch: 21 out of 182, Loss: 5.76\n",
      "thou art more than a\n",
      "to be or not to be\n",
      "wherefore art thou art a\n",
      "Iteration: 1660, Epoch: 10, Batch: 31 out of 182, Loss: 5.45\n",
      "Iteration: 1670, Epoch: 10, Batch: 41 out of 182, Loss: 5.55\n",
      "Iteration: 1680, Epoch: 10, Batch: 51 out of 182, Loss: 5.99\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-9e7a1b0d752d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         temp_loss, state, _ = sess.run([lstm_model.cost, lstm_model.final_state, lstm_model.train_op],\n\u001b[0;32m---> 20\u001b[0;31m                                        feed_dict=training_dict)\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "train_loss = []\n",
    "iteration_count = 1\n",
    "for epoch in range(epochs):\n",
    "    # Shuffle word indices\n",
    "    random.shuffle(batches)\n",
    "    # Create targets from shuffled batches\n",
    "    targets = [np.roll(x, -1, axis=1) for x in batches]\n",
    "    # Run a through one epoch\n",
    "    print('Starting Epoch #{} of {}.'.format(epoch+1, epochs))\n",
    "    # Reset initial LSTM state every epoch\n",
    "    state = sess.run(lstm_model.initial_state)\n",
    "    for ix, batch in enumerate(batches):\n",
    "        training_dict = {lstm_model.x_data: batch, lstm_model.y_output: targets[ix]}\n",
    "        c, h = lstm_model.initial_state\n",
    "        training_dict[c] = state.c\n",
    "        training_dict[h] = state.h\n",
    "        \n",
    "        temp_loss, state, _ = sess.run([lstm_model.cost, lstm_model.final_state, lstm_model.train_op],\n",
    "                                       feed_dict=training_dict)\n",
    "        train_loss.append(temp_loss)\n",
    "        \n",
    "        # Print status every 10 gens\n",
    "        if iteration_count % 10 == 0:\n",
    "            summary_nums = (iteration_count, epoch+1, ix+1, num_batches+1, temp_loss)\n",
    "            print('Iteration: {}, Epoch: {}, Batch: {} out of {}, Loss: {:.2f}'.format(*summary_nums))\n",
    "        \n",
    "        # Save the model and the vocab\n",
    "        if iteration_count % save_every == 0:\n",
    "            # Save model\n",
    "            model_file_name = os.path.join(full_model_dir, 'model')\n",
    "            saver.save(sess, model_file_name, global_step = iteration_count)\n",
    "            print('Model Saved To: {}'.format(model_file_name))\n",
    "            # Save vocabulary\n",
    "            dictionary_file = os.path.join(full_model_dir, 'vocab.pkl')\n",
    "            with open(dictionary_file, 'wb') as dict_file_conn:\n",
    "                pickle.dump([vocab2ix, ix2vocab], dict_file_conn)\n",
    "        \n",
    "        if iteration_count % eval_every == 0:\n",
    "            for sample in prime_texts:\n",
    "                print(test_lstm_model.sample(sess, ix2vocab, vocab2ix, num=10, prime_text=sample))\n",
    "        iteration_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XeYFFX28PHvGZAoSYIiUYKoLCsSRBREkooJ87IsZsX4\n80VdzArGBVZlFQNgdkXFgKtrWkkKioiA6KIgEkRASZLRVcJ5/6iqtrq7uru6ZzoMcz7PMw/dFW8X\nM/f0zaKqGGOMMckU5TsBxhhjCp8FC2OMMSlZsDDGGJOSBQtjjDEpWbAwxhiTkgULY4wxKVmwMMYY\nk5IFizJORLqIyAwR2SwiG0TkYxHpmO905YKIqIi0KMb5F4nIQhHZKiJrROQdEalWkmksRCJyjIis\nzHc6TG6Vz3cCTP6ISHXgLeBy4GWgAtAV+DWf6SoNRKQbcC9wvKp+LiL7ACfnOVnGZI2VLMq2AwFU\n9UVV3aWqv6jq+6r6pXeAiFwoIgtEZKOI/EdEmvj29Xa/WW8WkYdF5EMRudjdN1REnvcd29T9Jl/e\nfV9DRJ4UkR9FZJWI3C0i5dx954vIRyJyn3vfZSLSx3etfUTkaRH5wd3/L9++k0RknohscktMfwz6\n4CIyzX35hYhsE5E/udsvEZHFbinrTRHZP8Gz6wh8oqqfu89wg6o+q6pb3etUdNP/vVvqGC0ilX33\nH+x+9h/cZxwp5YjIB95z9D8P3/uDRGSim8ZvRORs375nROQREXnbLfF8KiLNfftb+85dIyI3u9uL\nRORGEVkiIj+JyMtuAEyL+//6nIisE5HlInKriBS5+1q4vyObRWS9iIx3t4uIjBSRtSKyRUT+KyJ/\nSPfeJrssWJRti4BdIvKsiPQRkVr+nSLSF7gZOB2oC0wHXnT31QEmALcCdYAlwFFp3PsZYCfQAjgM\nOBa42Le/E/CNe+0RwJMiIu6+fwJVgNZAPWCkm6bDgKeAS4HawBjgTRGpGHtzVT3afXmoqu6tquNF\npAfwN+BsoD6wHHgpQfo/BY4TkTtE5KiAewzDCcZt3c/YALjdTefxwF+B3kBLoFfCpxRDRKoCE4EX\n3M/eD3hURA7xHdYPuAOoBSwG7nHPrQZMAt4D9nfTNdk95/+AU4Fu7r6NwCNh0+UzCqgBNHOvdS5w\ngbvvLuB9N10N3WPB+b8/Gud51cB5/j9lcG+TTapqP2X4BzgYJ+NeiZN5vwns6+57F7jId2wR8DPQ\nBCcTmOnbJ+41LnbfDwWe9+1vCihO1ee+OFVdlX37/wxMdV+fDyz27avinrsfTia+G6gV8FkeA+6K\n2fYN0C3BZ1eghe/9k8AI3/u9gR1A0wTn9wH+DWwCtgEPAOXcZ7EdaO47tjOwzH39FDDMt+9Af1qA\nD7zn6HseH7mv/wRMj0nHGGCI+/oZ4AnfvhOAhb5n/HmCz7IA6Ol7X9/97OUDjj0GWBmwvRzwG3CI\nb9ulwAfu6+eAsUDDmPN64HxxOQIoyvffhP0E/1jJooxT1QWqer6qNgT+gPOt8h/u7ibAg26VziZg\nA05G2MA9boXvOup/n0ITYC/gR9+1x+B8U/as9l37Z/fl3kAjYIOqbkxw3eu8a7rXbeSmNYz9cUoT\n3n234XzDbRB0sKq+q6onA/sAfXEy9YtxSmFVgDm+dLznbvfu439WywmvCdAp5jP+BSeQelb7Xv+M\n89zAeRZLklz3dd81FwC7cAJ7WHVw/l/9n2c5vz+/63F+f2aJyFciciGAqk4BHsYpyawVkbHitKeZ\nAmLBwkSo6kKcb6ZeffEK4FJVren7qayqM4AfcTIfwKl39r/H+WZdxffen5mtwClZ1PFdt7qqtg6R\nzBXAPiJSM8G+e2LSW0VVXwxxXYAfcDJN7zNVxanOWpXsJFXdraqTgSk4z2498AvQ2peOGqrqZdpR\nzw5oHHPJVM/uw5jPuLeqXh7i863AqR5KtK9PzHUrqWrSzx5jPU5ppIlvW2Pc56eqq1X1ElXdH6fE\n8ajXTqOqD6lqe+AQnJLW4DTua3LAgkUZ5jaUXiciDd33jXCqKma6h4wGbhKR1u7+GiJylrvvbaC1\niJwuTqP11URnavOAo0WksYjUAG7ydqjqjzh11/eLSHW3cbW5OD2MknLPfRcno6klInuJiNf+8Dhw\nmYh0chtNq4rIiZK4O+saojPPF4ELRKSt2wZxL/Cpqn4X8Oz6ikg/Nw0iIofj1NHPVNXdblpGikg9\n9/gGInKce/rLwPkicoiIVAGGxFx+HnC6iFRxM9OLfPveAg4UkXPcz76XiHQUkYNTPTv33PoiMkic\nBvhqItLJ3TcauEfcDgwiUtdts0pIRCr5f3CqB192r1PNvda1wPPu8Wd5v2s4bSIK7HbT30lE9sIJ\nlP9zr2UKiAWLsm0rTkPypyKyHSdIzAeuA1DV14HhwEsissXd18fdtx44C6ch9yechtqPvQur6kRg\nPPAlMAcno/I7F6er7tc4GcerOPXkYZyD8w12IbAWGOTeczZwCU6Vxkacxt3zk1xnKPCsW/VytqpO\nAm4DXsP59t8cp7E4yEb3Xt8CW3AyxL+r6jh3/w3u/We6z24S0MpN57s4VX1T3GOmxFx7JE7d/xrg\nWcC7Jur0tjrWTdcPOFVOw4G4RvxY7rm9cbr4rnbT3t3d/SBOe9X7IrIV53ehU9B1XA1wSk/+n+Y4\nDeXbgaXARzgN8U+553TE+V3b5t7r/6nqUqA6TnDdiFNt9RPw91Sfx+SWOFXNxhSfiHyA06j9RL7T\nUtqIiAItVXVxvtNiTBArWRhjjEnJgoUxxpiUrBrKGGNMSlayMMYYk1Kpm0iwTp062rRp03wnwxhj\nSpU5c+asV9W6qY8MVuqCRdOmTZk9e3a+k2GMMaWKiKQzU0Acq4YyxhiTkgULY4wxKVmwMMYYk5IF\nC2OMMSlZsDDGGJOSBQtjjDEpWbAwxhiTUpkJFvPnz+e2225j7dq1+U6KMcaUOmUmWCxcuJC7776b\n6dOn5zspxhhT6pSZYFG+vDNY/cwzz8xzSowxpvQpM8GiXLly+U6CMcaUWmUmWIhIvpNgjDGlVpkJ\nFsYYYzJXZoKFLfJkjDGZKzPBwhhjTOYsWBhjjEmpzAQLq4YyxpjMlZlgYYwxJnNlJlhYycIYYzJX\nZoJFmzZt8p0EY4wptcrnOwG50qxZMzp27EitWrXynRRjjCl1ykzJAqBixYrs3Lkz38kwxphSp0wF\ni/Lly1uwMMaYDJSpYLF69WqmTZvGDz/8kO+kGGNMqVKmgsXChQsBePbZZ/OcEmOMKV3KVLDw3Hzz\nzflOgjHGlCplMlgYY4xJjwULY4wxKZXZYGEjuo0xJjwLFsYYY1IqU8Gibt26kde7du3KY0qMMaZ0\nyVqwEJFWIjLP97NFRAbFHHOMiGz2HXN7ttIDMHny5MhrCxbGGBNe1uaGUtVvgLYAIlIOWAW8HnDo\ndFU9KVvp8KtYsWLk9UcffUSvXr1ycVtjjCn1clUN1RNYoqrLc3S/QOXKlYu87t27Ny+++GIeU2OM\nMaVHroJFPyBRztxZRL4QkXdFpHXQASIyUERmi8jsdevWZZwIf7AAmDVrVsbXMsaYsiTrwUJEKgCn\nAK8E7J4LNFHVQ4FRwL+CrqGqY1W1g6p28DdSpys2WGzcuDHjaxljTFmSi5JFH2Cuqq6J3aGqW1R1\nm/v6HWAvEamTgzQBsHv37lzdyhhjSrVcBIs/k6AKSkT2ExFxXx/upuenbCXEgoMxxmQmqyvliUhV\noDdwqW/bZQCqOho4E7hcRHYCvwD9NIuj5WKDhaqyadMmatasma1bGmPMHkFK20jmDh066OzZszM6\n95dffqFKlSqR99WrV2fLli1Mnz6dLl26lFQSjTGm4IjIHFXtkOn5ZWoEd+XKlXniiSci77ds2QJY\nryhjjEmlTAULgB07duQ7CcYYU+pYsADcNnZjjDEJlLlgsXPnznwnwRhjSp0yFywqVaoUt81KFsYY\nk1yZCxY1atTIdxKMMabUKXPB4owzzojb9sUXX3DttdfagkjGGJNAmRpn4WnYsCGrVq2K275p0yYr\neRhj9kg2zqIEFRXZ4zDGmCBlMne0Bm1jjElPmQwWvXv3DtxuEw0aY0ywMhksRo8eHbjdgoUxxgQr\nk8GiQoUKgdt37dqV45QYY0zpUCaDRSJWsjDGmGAWLHwsWBhjTLAyGyx69OgRt82ChTHGBCuzwaJc\nuXJx2yxYGGNMsDIbLNq3bx+3zYKFMcYEK7PB4q677orb9tlnn+UhJcYYU/jKbLAoX7583LYzzzwz\nDykxxpjCV2aDhTHGmPAsWBhjjEnJgoUxxpiULFgYY4xJyYJFjEWLFuU7CcYYU3AsWMRYt25dvpNg\njDEFx4KFMcaYlCxYGGOMSanMB4t69erlOwnGGFPw4ocxlyGbN2+mfPnyVK1aNbJNVfOYImOMKUxl\numRRvXp1qlSpwj//+c/ItgULFuQxRcYYU5iyFixEpJWIzPP9bBGRQTHHiIg8JCKLReRLEWmXrfQk\nM2DAgMjrgQMH5iMJxhhT0LIWLFT1G1Vtq6ptgfbAz8DrMYf1AVq6PwOBx7KVnnT8/PPPTJs2Ld/J\nMMaYgpGraqiewBJVXR6zvS/wnDpmAjVFpH6O0pRQu3bt6NatG5MnT853UowxpiDkKlj0A14M2N4A\nWOF7v9LdFkVEBorIbBGZnYtBc94o7h9//DHr9zLGmNIg68FCRCoApwCvZHoNVR2rqh1UtUPdunVL\nLnEJVKhQAYDRo0eX6HVVlfnz55foNY0xJhdyUbLoA8xV1TUB+1YBjXzvG7rb8urXX38F4OOPPy7R\n6z755JO0adOGKVOm8OqrryIibNiwoUTvYYwx2ZCLYPFngqugAN4EznV7RR0BbFbVPbbu57333gOc\n+afuv/9+ABYuXJjPJBljTChZHZQnIlWB3sClvm2XAajqaOAd4ARgMU5vqQuymZ5827JlCwA1a9ZE\nRAAbBGiMKR2yGixUdTtQO2bbaN9rBa7MZhoKiRcYRMSChTGmVCnTI7jzxYKFMaa0sWCRQkn2XvIH\nBi9YGGNMaWDBIoWnn366xK/pDxpWsjDGlAYWLFJ44IEHWLNmDdu2bYtsW7JkCRs3bkz7Wl5g2L17\nN0VFRVHbjDGmkFmwCGG//fbj6KOPjrxv0aIFbdu2zfh6t956a2TuqWOOOYbVq1cXO43GGJNNFixC\n+vzzz6Pef//992lfwytFzJkzJ2r7W2+9lXnCjDEmByxYFABr7DbGFDoLFq53332XTp06ZfUe1j5h\njCmtyvSyqn7HH388VapUoVu3bjm/t9fYbYwxhcpyKZ9sf/NPdH2rhjLGFDoLFj5HHnlkymOKM+4i\nUbC44IILmDJlSsbXNcaYbLNg4bPXXnvRuXPnpMdceOGFWbn32WefnZXrGmNMSbBgEWPnzp0J98WW\nDL7//nsGDRrErl27in1fa/w2xhQyCxYxjjjiCACqV68ety92nqjzzz+fBx98kBkzZoS6drKAYMHC\nGFPILFjEuO+++5g3bx4HHHBA3L4FCxZEvfdKIbt37wZg06ZNLFmyJOG1LVgYY0or6zobo0KFChx6\n6KGhMu/YXkzt27dn6dKllvEbY/Y4VrIoAV5wWLp0adT2xx57jBdeeCHy/pdffkl5DWOMKURWskjD\nzJkzo96nWsDoiiuuAKB///5AcDuIx4KFMaaQWckiDSNHjox6/+GHH6Z1fo0aNRLu+/nnnzNKkzHG\n5EKoYCEizUWkovv6GBG5WkRqZjdppUfYUoHXEB5k586dccHIGGMKRdiSxWvALhFpAYwFGgEvJD+l\ndEunWijssanGY1x77bWh72mMMbkUNljsVtWdwGnAKFUdDNTPXrLyL902hIceeijyOlEJIszgvR07\ndvDAAw+wY8eOtO5vjDHZFDZY7BCRPwPnAd5KPXtlJ0mlj6py3XXXRd4nCgphgsXDDz/MddddFxV8\njDEm38IGiwuAzsA9qrpMRA4A/pm9ZOXfNddcE/rY2CCQaMqQMMHilVdeAWDr1q2h72+MMdkWKlio\n6teqerWqvigitYBqqjo8y2nLqwsvvDB0VdRNN90UNUCvSpUq3HHHHXHHxVZPffnll3HHfPLJJ4BN\nW26MKSxhe0N9ICLVRWQfYC7wuIg8kN2klR6x04AADB06NG5bbMki2aJHFiyMMYUkbDVUDVXdApwO\nPKeqnYBe2UtW4Zg9ezYvvvhi0mOuv/76lJn7/PnzmT59etS2ZMFiyJAhNvbCGFMwwgaL8iJSHzib\n3xu4y4T27dvTr1+/pG0IYaqr2rRpE7ctVYD5z3/+kzqBxhiTA2GDxZ3Af4AlqvqZiDQDvs1esgrP\n3nvvTbVq1QL37d69O6Nqo1Rrb1tVlDGmUISaG0pVXwFe8b1fCpyRrUQVqvLlgx9XspHZyaQKFsYY\nUyjCNnA3FJHXRWSt+/OaiDQMcV5NEXlVRBaKyAIR6Ryz/xgR2Swi89yf2zP9ILmQKFj87W9/49df\nf037eqlKDlayMMYUirCzzj6NM73HWe77Ae623inOexB4T1XPFJEKQJWAY6ar6kkh05FXe+1VsuMQ\nrRrKGFNahK0HqauqT6vqTvfnGaBushNEpAZwNPAkgKr+pqqbipXaPEvUZpHK7bf/XmCqX//3WVLK\nlSuX9DwLFsaYQhE2WPwkIgNEpJz7MwD4KcU5BwDrgKdF5HMReUJEqgYc11lEvhCRd0WkddCFRGSg\niMwWkdnr1q0LmeSSl2mwuOuuuyKvu3btGnmdqFrLo6osX748o3t6nn76aUSEH3/8sVjXMcaUbWGD\nxYU43WZXAz8CZwLnpzinPNAOeExVDwO2AzfGHDMXaKKqhwKjgH8FXUhVx6pqB1XtULdu0gJNVlWp\nElSLlp5y5coxbtw4Jk+enLJa6/7776dp06Z8/fXXGd/vqaeeAuDbb8tU5zVjTAkLO93HclU9RVXr\nqmo9VT2V1L2hVgIrVfVT9/2rOMHDf90tqrrNff0OsJeI1EnvI+ROnz59in2NoqIi+vfvT48ePVIG\ni2nTpgHw3XffZXw/b54qf+nGGGPSVZy+m0kXX1DV1cAKEWnlbuoJRH1FFpH9xK2YF5HD3fSkqt7K\nmxtuuIHTTz+9WNfwt0OkqobynHjiiXz88ccZ3c+b6nzSpEkZnW+MMVC8NbjDtL7+HzDO7Qm1FLhA\nRC4DUNXRONVZl4vITuAXoJ8W8GLUIkLz5s2LfQ1POr2runTpktE63WFmujXGmFSKEyxS5lyqOg/o\nELN5tG//w8DDxUhDznXoEPtx0pNOsCgqKgoc8Hf99dfTs2dPjjvuuJT3s2BhjCkJSauhRGSriGwJ\n+NkK7J+jNBaUs88+u1jn+8dWZDqC++9//zvHH398qGMTra2RLRs3bsxogKIxprAlza1UtZqqVg/4\nqaaqxSmV7BG8nkbpSCdApJpG5LHHHuOkk05i8+bNiAgXXXRR3DH+YDFnzpzwCc3A+PHj2WeffejZ\ns2dW72OMyT0p4CaCQB06dNDZs2fnNQ1eVZKqpj1w7qKLLuKJJ56Iu1YY3oSFsefcdtttkd5Osf+f\nzZs3Z+nSpZH3Qf/fq1evZseOHdSrV4+tW7dSp05mHdL86Sptv1fG7OlEZI6qZlyPbjPZ5VhsRn/X\nXXdFusimUlRUFNgbyz99uohwxhm/92oOqoaaOnUqr776auR9/fr1ady4MaeeeiolNY5FRCJLxPrN\nmDEDEWHGjBklch9jTG6U+aqkTAwZMoTt27dndO6KFSui3t96660AvPbaa1GZfCKvv/563LZt27ZF\nvZ8wYQLbt2+natWqgcGiR48eQPy3//feey/l/dPxj3/8g7POOitqm7dGx/vvv8+RRx5ZovczxmSP\nlSwyMHToUP7+979ndG6iBY1OP/30Ep2o8Prrrwfie0MFrfudLUFVUV6bjX9fq1ataNgw5STGxpg8\nsmBRgvxVOP3790/7/JJcF+Onn5yxjbEli0MPPTSje4ATeHr16sXUqVMBWLt2Lc8991zC44OChVcN\n5/+sixYtYtWqVWmn584778xp8DOmLLNgUUz+uv81a9ZEXp955plpXyvTMRFBVU3jx49n165dKa8Z\nZgqT5cuXM3jwYM477zwmT54cCYQDBgzgvPPOY/z48QwYMCBUWv2dA4pj165dDBkyhMMPPzxw/86d\nO+nZs2cksBljisfaLIrJa2fo3r173qYUT9SFd/Xq1SmDRVA7hary2GOPsd9++3H66afTr18/Zs6c\nGXfcpk3OjPP9+vULvHaykkVxg4V3fqIxHevXr2fKlCnMnz8/KogXirFjx9KsWTN69eqV76QYE4oF\nixKwYcOGyIy006ZNY8OGDXFVSmeffTYvv/xyXINvNqlqRoPyFixYwJVXXhm5hhcUPF6GX6NGjaTX\nmTVrVlz3Ym+q9OIGi1RVdl77j7+nmOfrr7+mZcuWJb6YVTouvfRSwLoYm9LDqqFKQK1atahYsSLg\nrFfRt2/fuGO8TCFZ9dQ999xToul65plnkgaLRPtiu7XGZmhe5p9qgKGqMmbMmKhtjz76KOBUIx17\n7LGMHDky6TUSSRUsvP2//PJL1Pbly5fTunVr/vrXv2Z0X2PKKgsWWRJbJeVlXsmqqm6++WaWLFlS\nYmm45557IrPOBnn55ZcDt8dmxInW8Qjzrfjrr79m165dDB48mNWrV0e2jxgxgokTJ3LttYknL966\ndSvvvvtuqDQuXbqULVu2RN4nqn5bv349AB999FHKtBtjfmfBIke8jDWX627/73//S7rfyzhjeT2p\nEkknjaNGjWLixIncd999XHbZZaHPA7jwwgs54YQTePbZZ/nss8+i9sUGi+bNm3PUUUcl3F9SPvvs\nM3777besXNuYQmbBIkfClCzC7C9JsYP5PDfffHPkdffu3fn888+LdR8vc003k120aBEA559/flyv\np6BgMH/+/KT7i2vx4sUcfvjhDBo0KLJt8+bNgYtTvfXWWxmPxTGmEFmwyJImTZpEvc9HySKVMI3f\nH3zwQdy2VatW8cUXX4S+TyYZ9//+97+4MRS7du3ixRdfZPfu3VHXHDVqVOh7FqdB+auvvgJg7ty5\nkW3t2rXjgAMOiDv25JNPjgyMNGZPYMEiSw477LBI5gKFWbIYMmRIxue2bds2dPuK136QTkZ9xx13\nxG179NFH6d+/P08++WRUMLj66qsjr7/55hsgdYBasGABc+fO5YcffgidplNPPRWI/j/yT9LosfXO\nzZ7IgkUWHXLIIZHXhViyKK6gjDJI2GAhIrz11lsAcd11gUgD+Zo1axIGg4MOOoh58+bx/fffJ73X\nL7/8Qvv27WnQoEHK9AelMxl/NVXQJJGxvc0WLFiQtTYWY0qKBYsse+edd/jqq68KsmSRK16wCDM1\nh9d9uFKlSnH7vGdz++2388gjjyS8xmGHHUa3bt0i7ydMmMAjjzyCiHDeeeclvX+YUkE6Ab9bt25R\n1VZAVEP8vHnzOOSQQxg2bFjK+xqTTxYssqxPnz4ccsghkWCRzuJH3tiN0m7evHnA7wPykvFKH0HB\nwn/M0KFDQ9//jDPO4KqrrgKcrrxB1/v11185/PDDOfDAAxN2KfasXbsWEQm9+FWyz+2VgIJGyIfx\n7rvvxs2rtWXLFnr37s3ixYszuqYxQSxY5EjYaii/XC+Jmi0jRoxI+5xkwaKkPfPMM/Tv3z/SPTe2\nJBDLK308/fTTgftjS4f+XmCxPdCKO4L7hBNOoFOnTlHbJk+ezKRJkyJjWF544QUWLlxYrPsYY8Ei\nR+6//366du1K165dQ59Trly5Ert/mAkDC8Gnn37K+PHjA0sO2aqimz17NhMmTIi8T9V+kO7a6f/6\n178AJ2hUq1Ytap93r+IEjdiSxb777gvAunXrAPjLX/7CwQcfnPH1w9qxYwd33nknP//8c9bvZXLP\ngkWOtG7dmmnTplG1atWkx/kzjRkzZnDyySendZ/y5YOn+ypNpZREExNmq5fR22+/HfU+7FQi/lHg\nf/vb3xIe703jHjTpoff/HRssli9fjojwySefsHLlyqiBkqrKP/7xDzZv3hx4P68kk+t5p5588kmG\nDBkS9Sy2b98euiOEKWwWLAqM9wfesGFD2rdvH7gyXjJ/+MMf4rb1798/6bQfhaZy5cqB28ePH5+V\n+y1fvjzqfSY9k26++WZUlR9++CFhCSioRJLoXpMmTQLgiSeeoFGjRjRu3Diyb+jQoVxzzTVcfvnl\nged2794dyH6wWLNmTVRbizfdyq+//oqqMn/+fE4++WSaN2+e1XSY3LBgUWC8P/Cwk/XFateuXdy2\ncePGlaqSRezkf7nmz2THjx8f2I03yKOPPkqDBg0Ce31NnDgxMPNOVA3lvfca0f1VO3feeSeQeLoW\nz6xZs5gyZUrC/T/99BNnnHEGFStW5PHHH096rSAdOnSgc+fOkfdeiaZChQqMHDmSNm3a2HoiexAL\nFgXKCxbp1tMnqg4pTcEi37wMfM2aNfTr1y90VaCXMcaWVACOPfbYpMHinXfeiZQmtm/fziWXXBI6\nncncdtttCffdd999TJgwgd9++42BAwcCzsj5sCWSlStXRr33B4tZs2ZF7Yu95vjx46OmZ8nUhg0b\naNasGffdd19G569duzbhmigmmgWLAlOcqoOGDRvGrdHg/QEvW7asWOkqS7xv7N74kI8++ihU0H7t\ntdeS7g+aG8uf4XtrXIRtmwkTLPy/T5s3b440tkP0FxER4ccff6Ry5co89NBDcdcREW688cak9/I+\n31577ZWwpOTp168fbdq0SZn+ZD7++GNq167NsmXLGDx4cEbX2HfffTnllFOKlY6ywoJFgYmthgrr\nlFNOYcWKFXHnecGjEFeLK1QvvPACUPLVYf4p1D3bt2+PvE63NJluafGkk07itNNO4/vvv+e///1v\n3O+EN+bj+eefDzx/+PDhDBw4MOG4Ea9drEKFCimDRaz33nsvcEJGz9tvvx038j229JKp999/v0Su\ns6ezlfIKjLf63Iknnpj0OP8KdHPnzuWwww4D9swR4Pkwa9Ysbr/99hK9ZlDvJa/6B8K1U/lnBN64\ncWPUvlY7rPcfAAAevElEQVStWnH22WcnPNfrvbVr1y7++Mc/Ru0TkUhX7WQllscff5y1a9dGlVA8\nXrAI6pG3e/fupF3B+/TpQ40aNRK2D5100klAdNAJUwp/4oknaNOmTdxYFJM+CxYFZp999mHFihXs\nt99+oc/xBwh/RmMDsTKXjczFP5YjSJiShb9NKrbOf9GiRdx9991R24Iy1C5dugRe28vMYxeOir3G\nG2+8EZjGZKXi3bt3s2HDBqpVqxZVVdqtW7fIzMaJugInEiZYeG0/mVbvTp8+nZUrV/LnP/85avvY\nsWM54YQTaNiwIeAs1FW1atW0O6SUJnvuJyvFGjZsmHC8RCr+P9RWrVolPTZRF1WTHXfddVfS/Zn2\ngEsmaMXAoJl2/SWL2HNStY0EZcSx23bt2kXt2rXp3Lkzp512WmT7tGnToqr7HnjgAUaPHp0yvYnu\nm44wbT5HH300/fv3jzr2+++/59JLL42U4jZs2ED16tUDZ0oOsnXrVo444ojAqWcKmQWLUuDII49M\nuj+2oTLIxIkT47YddNBBxUuYKVHe/12iTPCNN95IeG6igYxhx4wkCxaJlqhNdA8RiWsP8VZtnDNn\nTlwVlr9b8HXXXRc1fuTNN99MODNwqmCRKjNO9bn8/CW6tWvXAr9/Ju+zpppTzLvngAED+PTTT7nl\nlltC378QZDVYiEhNEXlVRBaKyAIR6RyzX0TkIRFZLCJfikj8IAHDxx9/zCOPPBJqgF6ib6W9evWK\n25bL+ZdMat988w1z585NmIl562kESTRgMeyaIyISCVbpBotbbrklKuObOnUq06dPjzom0aqM8Huw\nCJo4M/Y6fqmCRevWrZPuTydY+L9sedVlNWrUYMqUKZGlCMKUCB9//HHefPNNwAmcngkTJnDvvfey\nbNmylMsh50u2SxYPAu+p6kHAocCCmP19gJbuz0DgsSynp9S64oorEmYW/lHb6TRwx3azLSnJpr4w\nyQ0ZMoRDDz20xK4XdkDhjh07ItUqsSWFVKWT4cOHc++990bex66XDsmDhderK6gBPHbmAX9PrUTB\nYtGiRUkHI3rSCRb+QOadt9deezFy5MjI9jDBwpuvC2DFihWRUsoZZ5zBLbfcQrNmzeLaRwpF1oKF\niNQAjgaeBFDV31Q19je3L/CcOmYCNUWkfrbStKcZOHAggwYNivojSxYsYrs8ZtIu4l8nIki7du24\n4IILEu5v37592vcsS7zFn/LBW9nRywx3797NTTfdFHo1QS/zjh2sB8mDRbJgFDs25ZxzzomkMdG4\nj1atWtGzZ8+U6c00WPiXG/AHs6KiIqZOnRrVFXfq1KlRSxPHBrig5xI7V1nQefmQzZLFAcA64GkR\n+VxEnhCR2Fn0GgArfO9XutuiiMhAEZktIrP9kbmsGzNmTNQ3G0j+7Sa2h1VssEiWyceeM2DAAAAa\nNWoUNWJ5zpw5kVlPgySrdzeFYdeuXagqnTp1YtiwYZx//vmhzku2IFWyMSvpBAtwMuCg0ssdd9xB\nOvlDOsHCX2XrDxb+sS5FRUX06NGD4447LrKtR48ekbm6ID7TT9RzzG/btm0UFRVlNNV/ScpmsCgP\ntAMeU9XDgO1A8iGgCajqWFXtoKod6tatW5Jp3OOkUw0VGyyeeuqpyBoIqc6pWbMm4PTc8k9yl0qD\nBg2iVoozhef7779n8ODBzJ49G6BEphzv3bt3wn3JvjUHTYDZo0ePqDmpPEOHDqVevXqh0xTU6SOR\noGqo2JJFmCUFwgxWjN3mDZZMtH5KrmQzWKwEVqrqp+77V3GCh98qoJHvfUN3m8lQ2GDRtWvXwGqo\nwYMHc/zxxyesorrrrrvo2LFj5I8/k6osfxr9jXzJ1K5dO+37mMzdf//9kdfpfANPJNmsx4mWHP78\n888j07sXl3+6m6+//pq33nqLP/3pT6HPf+GFF9h7771ZuHBhqGCxYsWKuGsAobrXxgYLb/Cl9wUt\nX7IWLFR1NbBCRLzO/j2B2L5sbwLnur2ijgA2q2rqtTfLsDvvvDNhN0lI3cg2duxYTjzxRD744AMq\nVKgQt3+//fbj3XffTfiLedBBBzFr1iz23ntvILMFmvztHkFTqgdJ1bPFZE9JBItkgqqhhg0bFjiD\ncqaaNWsWed26deu4ySHffvvtqLaFoDRu376dgw8+ONLDLKjNwjNmzJjQbT2xYoOFV4VXpUqVjK5X\nYlQ1az9AW2A28CXwL6AWcBlwmbtfgEeAJcB/gQ6prtm+fXs1yQHq/Ncm98MPP+gll1yi77//vi5e\nvDhqX+3atSPX8f9s27ZNVVUnTpyogPbs2TPwnkHnevs3b94ceb979+7I64MOOijheV27dk24L52f\n++67T8eNG1ci1yorP8n+X0riZ8qUKQpo1apVs3qfVL+b3jEbNmzQ9evXJzz2mmuuUUBPP/10bdu2\nbWT7kUceGXVclSpVUt57yZIlgdv9Jk2apIB27949XAaQADBbi5GfZ3W6D1WdB3SI2Tzat1+BK7OZ\nBpNY/fr1GTt2bOA+TVCPHNsXP5OShf8bmL/qoUGDBgmnKPG3Vb3xxhv07ds37fuCM+gLnKVGTTjZ\nLln06NEDiJ5UMZ/22WcfwPkbqFatGlu3bo3a7zVqFxUVRbXnxJbqY9t6gv6mVJVFixbFbf/5558j\nJYlUgzVzxUZw74GaNm2a0WI2fl4bQewvsvcHEds3/r333gucXC5IonaVRIMER48eHTVtdqbtF9la\naW9Pl63lbEuDoM4bo0aNApzfY/9Mwv5ldoMEBd3Vq1cHTstzzDHHxG2zYGFK3LJly7j44ouLdY1J\nkyYxZswYWrZsGdVDKlHJ4rjjjgv9bT9RsPDWcwja7s3Gmy5/12KbkbdsS9SZolatWoHbP/7446QZ\n9OzZs+NKHckETSmfaHZpf9dgLw0ffvghkydPDn2/kmbBwgRq3LhxZPpsf88Yr2Thjdlo27Zt2teO\nzbSHDh1KjRo1Agf8eaWNTCfXO/HEEzn99NPjruH/TKZs6NChQ2A1p7+K88EHH4y87tKlS9JgsWzZ\nstBrnnTp0iVwzEiYmXb9afBWU8wHCxYmlNjpsw8//HBmzJjB0KFDM76WZ8iQIWzatCnwm783Dbc/\no/dPUxKmui2oa6YXQDLlTdNgSpeffvopbpv/d2vQoEFR+1JV/YSdqPHjjz+OW0MkjC1btkQNNMzW\nFD1hWLAwoXjz9vsz3M6dOyds4A6qv/Ua0xOVEoK2N2rkDMPx38c/QCrVH7PXkyM27U2bNmXRokWc\ne+65Sc9PxAaHlk7+xaM8yUqtYYNBGEFrsyfz+eefU6NGDfr37x/ZZsHCFLyPPvqI559/PnTvp9jx\nEw0aNIgsRJOo7SDoj9bb5t/XqlUr2rRpwwcffBCq0c8/PYNfy5Yt8/rHZ3Jv2rRpcduSBYt8NioH\njTPJdJ2bkmDBwoTSuHHjYnU39X9DSxQsgv4wgxYEqlSpEl9++WXKSQ29a1arVg0IngK7pBq9vWmq\nS4Njjz0230koKMmCRaH1BLOShdnjhQkWlStX5qabboralmqp0VRrcqgqDz/8MPfcc0+oTDLV2ueJ\neDO2xgqawyjfMhkbsycrTUuhWrAwe6yqVaty3HHH8dJLL0W2Jcr4RSRqXYSRI0fG/SHfcMMNUe/9\n9bmJBj3VqlWLm2++OTBTiD3n1VdfTfhZYrvvBk2X4jd16tTIokDewLNcStTN0puqxTgsWISTvwow\nUyYUFRXx3nvvRW0LW/XjX14TgoNBqjrcRH3ogyxZsiSt1QNnzZoV2HX4o48+omHDhjRp0gRwRuNW\nrlw55+M8Es0l5HUaMI7SNP7GShZmj+Nl4m3atInbF/aPs7iNeUuWLIlbwyORq666KmqyuSCx6Y5d\n0W7UqFHUr1+fo446KhIowKley4dEga80fZPOBa97dmlgwcLscapWrcqUKVOKtfJbcerWTz755JSZ\nv1+YpUxTTWl91VVXZTzTaEkbPnx4wgGT2QwW+QqMxfHrr7/mOwmh5bO9yYKFyZru3bsnrQaqWjV2\n4cTMHH300VHvr776at58881Q56bTNfLhhx9OK1355K3j/OCDD8Y9i6KiItavXx9536VLl4zvs23b\ntqgJ8xJN2WJKPwsWJi8ef/xx5s6dWyLX+vDDDyOZvqpGTdkQlr+Kad68eSxcuDButG+iarErrrgi\n1D0SzagbJk3p8koPV199ddzaDdWrV6d27dqRrsS33npr1P50BqJVrVo1qjRREoPYunbtGhXMzO/y\n2b5iwcLkxcUXX8yBBx6Y72QEOvTQQ2nVqlVkqupkVDXp2tN+rVq1ClxgJ7Zd5eCDD2bnzp3F6rWU\nKFNp06ZNZGJIL6DEVm0UJ0MqiUFstWvXzmhm4Uwnm/TLtOt0rliwMKbAvfzyy1m7dq9evaLeiwjl\nypVj2rRpXHPNNSm76AZJ1C4xePDgSIkiUbAojnyNeD7iiCOKPTCyRYsWvPHGGyWUoj2PBQtTcGbN\nmsXo0aNTH1gCwn5TO+uss0rkfkGZaaLJENu2bcsDDzwQmEZ/Bh/U4yxMI7Z3jTDBol69eimPgeTV\nUN27dw81AWMmAeeTTz4pdk+hn3/+mXLlyhX0wlj5LFnYOAtTcDp27EjHjh1zcq9hw4YB0YP7siko\nE69UqRJ33XUX5cqVC5zoLkjNmjUjbSq33XYbZ599dtT+MJlKOsGiTZs2VK5cOWXvtlQZfdiqvUwU\nd0U/b7rxfC8ylIxVQxmTJ3Xq1OGJJ57IWZfPLl26cMMNN/Ddd99Fbb/11ls55ZRTAs/xMojrrrsu\nMsJdRBgxYgQABxxwQNw5iUoW/swmnS60yUa2+3kZbc+ePeP27d69O6vddovbuO4tThQmWMT2wAvj\nxhtvTPucQmLBwpg0rF+/Pmp9gXQVFRUxbNiwqEF7qZx00kkA3HvvvZxzzjmR7X/961/59ttvOeyw\nwwLv4xe0TKd3TFDmGDtGpWbNmqG+1XoZdtBgSFVFRNi2bVvSa2T6zb64wcIrmYS5TiZBL5P1LGJZ\nycKYUqJ27drUqVMnK9du0aIF7dq149FHH43a/vzzz7Ns2TIqVKgQyaREBBGhRYsWgRlXbKbSoEGD\nuGO86id/5uyVeJYsWcLxxx8POEEpLH8X5lheJly1alWWL1/O888/n/QasVq2bBm4XnWq88LygkWY\n68Q+8zBjhgq5eisMCxbGJDFr1iymT5+ek3tVrFiROXPmxE29XrFiRZo2bQoEf6MN+rYZmzEFZVQj\nRoxARKIWcvKXeLxxJV27dg39GZJliP5v7I0bN458prDX6NixY9KxKiVVsgiTqfvbeYIW+gqSKH13\n3HFHqPMhvf+LkmbBwpgkOnbsWKwRziUtbDVEogzPf/4555zD7t27E84h5fUu8uryPY0aNaJr165x\nEz3C7xlispJFUFqCjBs3jtGjR0eqtGrWrBl3zC233MKMGTOA4jdwZ1qyCDsmxP/5p0yZEnffMIJK\niLliwcKYUig2o1XVpGMEvAFrQYEhUabtlSx27NgRddyoUaOYNm1aXHWZlw7/v36xwSJRLyzv3P79\n+3PppZeyatUqRo4cGWnQ9zvooIMia4YUt2SRLO2xzj///MjrTAJ49+7dI6+LG+RyxYKFMXnyzDPP\nMH78+LTOSZaRnXLKKXz77bcMGzYsrovqiBEjGD58OKeddlrcebElB483tiJ2UGBQGp555hluvfVW\nevfuDQRPzBibmXfs2DFUFVpRURGDBg0KbBfwH+tdf+rUqdx///2BvcTCCBMs+vbtG7l+smDRq1cv\nHnvsMYCE7S3+55/u70Mu2TgLY/LkvPPOy/jcRBlUixYt4haIAmfBo+uvvz7wnESzrg4fPpzmzZvT\nt2/fpPeE6M/Sp08fZs6cGXdMbLDweobFpjedhuCgYFG7dm2uvfZarr322ox6D6XbZlFUVJSwVLP3\n3ntz6aWX0qdPn4Q94Dp06BB53a5dO8466yxeeeWVNFOdfVayMKaM876xX3DBBXHbr7nmmrhG9VSZ\nac2aNTn66KPp1asXw4cPj2wPylCDqqJSXX/WrFkcd9xxcdf0zgvqBHDVVVclvWY694fodItIwtKZ\n1104WVfpM888M/K6qKiI22+/PXRac8mChTGlSDa6XzZp0oSZM2dGqksSSedbepUqVZg4cWJU1UtJ\nBYuOHTtSv379uGt6r4OCRZgM+NRTTw11f3DS7X8eyYJFGI0bNwactBfq4lSFmSpjTCBvEsCDDjqo\nRK/bqVOnyLUT8aZi79SpU+jrBmXmfkHBIqiXVaygAYVeNVvDhg1D3SeW116QqqE8aMqSREEh9r7l\nypUL7D3lnS8iFiyMMcW3zz778M477/D666/n/N69e/dGVdl///1Dn3PUUUdFXgdlwrFTgs+aNSvy\nDT8ZbyyKf6bZCy64AFWlWrVqcceHCRaxDfn9+/fnyy+/THlebKDwSj0QX8rZtm0bq1atSngNb7Bl\nIbJgYUwp06dPn6QrEBaSevXq8dVXXwHBwaJZs2aoKu3atQPCT6Nx7rnnsnLlyki32VT8wcK/PO6E\nCRPijvUy7n79+gXO6JvMuHHjePbZZyPvYz9PpUqVAktwiUoW3bp1Y82aNWmlIVuyGixE5DsR+a+I\nzBOR2QH7jxGRze7+eSJSmC07xpiMeZlfsuodr2omVVWYXzoD1PwZsDcJ4AknnBDYldhLp/cN/777\n7osaxZ8sQPXr1y9q8GDYtUISBYsDDjiAevXqMWLECK655ppQ18qWXHSd7a6qydZInK6qJ+UgHcaY\nPPBKQclGwo8bN46XX36ZP/zhD1lJgz8D7t27N5s2baJKlSqBx/ozbnBm+/V76aWXovbH3sffphG2\npJQoWHj3GDx4cKjrZJNVQxljsmrfffflq6++Chzx7albty5XXnll1tJQqVIlHnnkERYvXkzLli2p\nUaNGwsWSYoOF37777hu43O0tt9wSed28efNIZ4B0VyFMFCwKQbaDhQLvi8gcERmY4JjOIvKFiLwr\nIq2DDhCRgSIyW0RmF2d6aGNMfhxyyCFpVTFlwxVXXEHz5s1THpcoWMyYMYN58+YFHn/33XdHNXR7\nA+3SrYaKvW8hzVSb7WqoLqq6SkTqARNFZKGqTvPtnws0UdVtInIC8C+gZexFVHUsMBagQ4cOhfP0\njDEFrVatWmzcuDGtc7yFsLy5sTxhG9Ph9/meilsNtXXr1tD3zLasBgtVXeX+u1ZEXgcOB6b59m/x\nvX5HRB4VkTop2jiMMSaUL774Ium05kHGjBlD69atA1f7S1fYdcETBYvNmzcXOw0lJWvBQkSqAkWq\nutV9fSxwZ8wx+wFrVFVF5HCcarGfspUmY0zZ0qhRIxo1apTWOfXq1ePuu+9OeVyyQDBgwAA+++wz\n7rnnnlD3TBQsHn744VDn50I2Sxb7Aq+79W/lgRdU9T0RuQxAVUcDZwKXi8hO4BegnxZSJZ0xxiTw\n73//m8cff5wWLVrE7atUqRJjxowJfa369euzbt06ypcvHzV1yIEHHlgiaS0JUtry5g4dOujs2XFD\nNowxJiNeg3Ku80L/fX/44QcmTZrEueeey7p166hXrx41a9ZMu70lxf3mqGqH1EcGs66zxhiTZ/vv\nvz/nnnsuEDzvVSGw9SyMMWXa+vXr+e233/KdjAibSNAYYwpQ7dq1oyb/y5Wg2XEhf9ViqVjJwhhj\n8mDp0qUFFxCSsWBhjDF5EHYMRqGwaihjjDEpWbAwxpgC4jVwJ5oVN18sWBhjTAGpXr06w4YN48MP\nP8x3UqJYm4UxxhSYG264Id9JiGMlC2OMMSlZsDDGGJOSBQtjjDEpWbAwxhiTkgULY4wxKVmwMMYY\nk5IFC2OMMSlZsDDGGJNSqVspT0TWAcszPL0OsL4Ek5MrpTHdlubcsDTnRmlMM0Snu4mq1s30QqUu\nWBSHiMwuzrKC+VIa021pzg1Lc26UxjRDyabbqqGMMcakZMHCGGNMSmUtWIzNdwIyVBrTbWnODUtz\nbpTGNEMJprtMtVkYY4zJTFkrWRhjjMmABQtjjDEplZlgISLHi8g3IrJYRG7Md3o8ItJIRKaKyNci\n8pWI/D93+1ARWSUi89yfE3zn3OR+jm9E5Lg8pfs7Efmvm7bZ7rZ9RGSiiHzr/lvL3S4i8pCb5i9F\npF0e0tvK9yznicgWERlUiM9ZRJ4SkbUiMt+3Le1nKyLnucd/KyLn5SHNfxeRhW66XheRmu72piLy\ni++Zj/ad0979vVrsfi7JcZrT/n3IZd6SIM3jfen9TkTmudtL9jmr6h7/A5QDlgDNgArAF8Ah+U6X\nm7b6QDv3dTVgEXAIMBT4a8Dxh7jprwgc4H6ucnlI93dAnZhtI4Ab3dc3AsPd1ycA7wICHAF8WgC/\nD6uBJoX4nIGjgXbA/EyfLbAPsNT9t5b7ulaO03wsUN59PdyX5qb+42KuM8v9HOJ+rj45TnNavw+5\nzluC0hyz/37g9mw857JSsjgcWKyqS1X1N+AloG+e0wSAqv6oqnPd11uBBUCDJKf0BV5S1V9VdRmw\nGOfzFYK+wLPu62eBU33bn1PHTKCmiNTPRwJdPYElqppsJoC8PWdVnQZsCEhPOs/2OGCiqm5Q1Y3A\nROD4XKZZVd9X1Z3u25lAw2TXcNNdXVVnqpOjPcfvn7PEJXjOiST6fchp3pIszW7p4GzgxWTXyPQ5\nl5Vg0QBY4Xu/kuQZcl6ISFPgMOBTd9NVbhH+Ka/agcL5LAq8LyJzRGSgu21fVf3Rfb0a2Nd9XShp\n9vQj+g+qkJ+zJ91nW2jpvxDnG6znABH5XEQ+FJGu7rYGOOn05CvN6fw+FNJz7gqsUdVvfdtK7DmX\nlWBR8ERkb+A1YJCqbgEeA5oDbYEfcYqXhaSLqrYD+gBXisjR/p3uN5aC65ctIhWAU4BX3E2F/pzj\nFOqzTUREbgF2AuPcTT8CjVX1MOBa4AURqZ6v9MUodb8PPn8m+ktQiT7nshIsVgGNfO8butsKgojs\nhRMoxqnqBABVXaOqu1R1N/A4v1eBFMRnUdVV7r9rgddx0rfGq15y/13rHl4QaXb1Aeaq6hoo/Ofs\nk+6zLYj0i8j5wEnAX9wgh1uV85P7eg5Onf+Bbvr8VVU5T3MGvw+F8pzLA6cD471tJf2cy0qw+Axo\nKSIHuN8s+wFv5jlNQKSe8Ulggao+4Nvur9M/DfB6P7wJ9BORiiJyANASp7EqZ0SkqohU817jNGTO\nd9Pm9bo5D3jDl+Zz3Z47RwCbfVUquRb17auQn3OMdJ/tf4BjRaSWW5VyrLstZ0TkeOB64BRV/dm3\nva6IlHNfN8N5tkvddG8RkSPcv4tz+f1z5irN6f4+FEre0gtYqKqR6qUSf87ZarUvtB+cXiOLcKLr\nLflOjy9dXXCqFL4E5rk/JwD/BP7rbn8TqO875xb3c3xDFnuLJElzM5xeH18AX3nPE6gNTAa+BSYB\n+7jbBXjETfN/gQ55etZVgZ+AGr5tBfeccYLZj8AOnPrkizJ5tjjtBIvdnwvykObFOPX53u/1aPfY\nM9zfm3nAXOBk33U64GTQS4CHcWeZyGGa0/59yGXeEpRmd/szwGUxx5boc7bpPowxxqRUVqqhjDHG\nFIMFC2OMMSlZsDDGGJOSBQtjjDEpWbAwxhiTkgULs8cTkX1F5AURWepOT/KJiJyWp7QcIyJH+t5f\nJiLn5iMtxqSjfL4TYEw2uYOO/gU8q6r93W1NcKb8yNY9y+vvE+jFOgbYBswAUNXRCY4zpqDYOAuz\nRxORnjhTNncL2FcOGIaTgVcEHlHVMSJyDM5U1euBPwBzgAGqqiLSHngA2Nvdf76q/igiH+AMfuqC\nM3BqEXArzrTVPwF/ASrjzL66C1gH/B/ODLjbVPU+EWkLjAaq4AyWulBVN7rX/hToDtTEGYg1veSe\nkjGpWTWU2dO1xhm9GuQinOkxOgIdgUvcqRzAmf13EM46Bs2Ao9w5vEYBZ6pqe+Ap4B7f9SqoagdV\nvR/4CDhCnUncXgKuV9XvcILBSFVtG5DhPwfcoKp/xBlFPMS3r7yqHu6maQjG5JhVQ5kyRUQewfn2\n/xuwHPijiJzp7q6BM3/Ob8AsdefZcVceawpswilpTHQXFiuHM/WCZ7zvdUNgvDvXUAVgWYp01QBq\nquqH7qZn+X1mXIAJ7r9z3LQYk1MWLMye7iucOXIAUNUrRaQOMBv4Hvg/VY2aYM+thvrVt2kXzt+K\nAF+paucE99ruez0KeEBV3/RVaxWHlx4vLcbklFVDmT3dFKCSiFzu21bF/fc/wOVu9RIicqA7i24i\n3wB1RaSze/xeItI6wbE1+H3aZ//611txls+NoqqbgY2+BWrOAT6MPc6YfLFvKGaP5jZKnwqMFJHr\ncRqWtwM34FTzNAXmur2m1pFkeUlV/c2tsnrIrTYqD/wDp/QSayjwiohsxAlYXlvIv4FXRaQvTgO3\n33nAaBGpgrNm9gXpf2JjssN6QxljjEnJqqGMMcakZMHCGGNMShYsjDHGpGTBwhhjTEoWLIwxxqRk\nwcIYY0xKFiyMMcak9P8B1MKHUtObNZcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117b006d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss, 'k-')\n",
    "plt.title('Sequence to Sequence Loss')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
